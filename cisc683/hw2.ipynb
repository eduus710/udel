{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# computation methods for decision tree\n",
    "# set of instances represented by pandas dataframe \n",
    "# (makes computations and groupbys over set easy)\n",
    "\n",
    "# compute entropy of instances / class_val\n",
    "def entropy(df, class_val):\n",
    "    freqs = df[class_val].value_counts(normalize=True)\n",
    "    return -(freqs * np.log2(freqs)).sum()\n",
    "\n",
    "# compute gain of a instances / class_val over attribute\n",
    "def gain(df, class_val, attribute, current_entropy):\n",
    "    weighted_entropy = 0\n",
    "    for attr_val, df_split in df.groupby([attribute]):\n",
    "        weight = df_split.shape[0] / df.shape[0]\n",
    "        split_entropy = entropy(df_split, class_val)\n",
    "        weighted_entropy += weight * split_entropy\n",
    "        print('*****', attribute, attr_val, weight, split_entropy)\n",
    "    print('****', attribute, 'weighted_entropy', weighted_entropy)\n",
    "    return current_entropy - weighted_entropy\n",
    "        \n",
    "# determine best split attribute of instances / class_val\n",
    "def find_split_attr(prefix, df, class_val):\n",
    "\n",
    "    (best_attr, best_gain) = (None, -1)\n",
    "    base_entropy = entropy(df, class_val)\n",
    "    print('*** base_entropy', prefix, base_entropy)\n",
    "    \n",
    "    # dirty check for zero\n",
    "    if (base_entropy < 0.000001):\n",
    "        return None\n",
    "\n",
    "    # dirty way to ignore attributes we've inspected\n",
    "    ignore_attrs = [x for x,_ in prefix]\n",
    "    ignore_attrs.append(class_val)\n",
    "    \n",
    "    for attr in list(df.columns):\n",
    "        if attr in (ignore_attrs):\n",
    "            continue\n",
    "        print('**** evaluate', prefix, attr)\n",
    "        this_gain = gain(df, class_val, attr, base_entropy)\n",
    "        print('****', attr, 'gain', this_gain)\n",
    "        if (this_gain > best_gain):\n",
    "            (best_attr, best_gain) = (attr, this_gain)\n",
    "            \n",
    "    if best_attr is None:\n",
    "        return None\n",
    "    \n",
    "    print('*** choose', best_attr, best_gain)\n",
    "    # return (best_attr, best_gain, [[prefix, split_df], ...])\n",
    "    return (best_attr, best_gain, [[prefix + [(best_attr, split_val)], split_df] for (split_val, split_df) in df.groupby([best_attr])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import CSV\n",
    "# iteratively split the instances on entropy-minimizing attributes\n",
    "\n",
    "df = pd.read_csv('./hw2-data-data.csv')\n",
    "class_val = 'HOUSEPET'\n",
    "\n",
    "# initial split evaluation to seed stack of nodes for evaluation\n",
    "tree = []\n",
    "prefix = []\n",
    "print('examine', prefix)\n",
    "result = find_split_attr(prefix, df, class_val)\n",
    "splits = result[2]\n",
    "tree.append(([], result[0], ('gain', result[1])))\n",
    "\n",
    "# while nodes remaining to split\n",
    "while splits:\n",
    "    (prefix, df_split) = splits.pop()\n",
    "    print('examine', prefix)\n",
    "    result = find_split_attr(prefix, df_split, class_val)\n",
    "    if result is None:\n",
    "        class_val_counts = df_split[class_val].value_counts()\n",
    "        tree.append((prefix, ('count', class_val_counts.index[0], class_val_counts[0])))\n",
    "    else:\n",
    "        tree.append((prefix, result[0], ('gain', result[1])))\n",
    "        splits.extend(result[2])\n",
    "\n",
    "# dump the tree\n",
    "tree.sort()\n",
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(s):\n",
    "    g = 1\n",
    "    for i in s:\n",
    "        g -= ((i)/ sum(s))**2\n",
    "    return g\n",
    "\n",
    "def weighted_gini(s1, s2):\n",
    "    print('***', s1, s2)\n",
    "    w1 = sum(s1) / (sum(s1) + sum(s2))\n",
    "    w2 = sum(s2) / (sum(s1) + sum(s2))\n",
    "    g1 = gini(s1)\n",
    "    g2 = gini(s2)\n",
    "    print(f'*** weight1: {w1} gini1: {g1} weight2: {w2} gini2: {g2}')\n",
    "    return ((w1 * g1) + (w2 * g2))\n",
    "\n",
    "def weighted_gini_partition(attr, part_vals):\n",
    "    value_counts1 = df[df[attr].isin(part_vals)][class_val].value_counts()\n",
    "    value_counts2 = df[~df[attr].isin(part_vals)][class_val].value_counts()\n",
    "    return (weighted_gini(value_counts1.tolist(), value_counts2.tolist()))\n",
    "\n",
    "\n",
    "attr = 'SHOTS'\n",
    "parts = ['all', 'some', 'none']\n",
    "for part in parts:\n",
    "    print(f'\\nshots == {part}')\n",
    "    print ('gini:', weighted_gini_partition(attr, [part]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
